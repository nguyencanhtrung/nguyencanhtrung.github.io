I"˜J<h2 id="1-gi·ªõi-thi·ªáu">1. Gi·ªõi thi·ªáu</h2>

<p>N·ªëi ti·∫øp ph·∫ßn 1, ph·∫ßn n√†y s·∫Ω m√¥ t·∫£ chi ti·∫øt v·ªÅ vi·ªác c√†i ƒë·∫∑t m√°y ·∫£o KVM. Ph·∫ßn h∆∞·ªõng d·∫´n n√†y s·∫Ω ƒë∆∞·ª£c m√¥ t·∫£ b·∫±ng ti·∫øng Anh.</p>

<p>Splitting IOMMU is required in this case, however my machine which includes a MOBO (Z390 Gigabyte Wifi Pro + CPU 9900K) does not support <code class="language-plaintext highlighter-rouge">pcie_acs_override</code>. Therefore, even putting the grub command like above, the IOMMU is not splitting as expectation. To make it works, we have to create a Patched ACS kernel and running this kernel instead. Step 5 shows how to do so.</p>

<h2 id="2-prerequisites">2. Prerequisites</h2>

<p>Before getting started, ensure that you meet the following prerequisites:</p>

<ol>
  <li>
    <p><strong>Linux System</strong>: Ubuntu 20.04 - kernel v5.15.</p>
  </li>
  <li>
    <p><strong>Bios</strong>: Make sure Intel Virtualization Technology (Intel VT) and Intel ¬Æ VT-d must be enabled from server BIOS</p>
  </li>
  <li>
    <p><strong>CPU Support</strong>: Check if your CPU supports virtualization and IOMMU (Input-Output Memory Management Unit) by running:</p>
  </li>
</ol>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>egrep -c '(vmx|svm)' /proc/cpuinfo
</code></pre></div></div>
<p>If the command returns a value of 0, your processor is not capable of running KVM. On the other hand, any other number means you can proceed with the installation.</p>

<p>You are now ready to start installing KVM.</p>

<h2 id="3-kvm-installation">3. KVM Installation</h2>

<h3 id="a-install-kvm-and-assorted-tools">a. Install KVM and assorted tools</h3>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt update
<span class="nb">sudo </span>apt <span class="nb">install </span>qemu-kvm libvirt-clients libvirt-daemon-system virtinst bridge-utils cpu-checker virt-viewer virt-manager qemu-system
</code></pre></div></div>

<h3 id="b-check-whether-your-system-can-use-kvm-acceleration">b. Check whether your system can use KVM acceleration</h3>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>kvm-ok
</code></pre></div></div>

<p>The output should look like this:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tesla@tesla:~/kvm<span class="nv">$ </span>kvm-ok
INFO: /dev/kvm exists
KVM acceleration can be used
</code></pre></div></div>

<p>Then run the virt-host-validate utility to run a whole set of checks against your virtualization ability and KVM readiness.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>virt-host-validate
QEMU: Checking <span class="k">for </span>hardware virtualization                                 : PASS
QEMU: Checking <span class="k">if </span>device /dev/kvm exists                                   : PASS
QEMU: Checking <span class="k">if </span>device /dev/kvm is accessible                            : PASS
QEMU: Checking <span class="k">if </span>device /dev/vhost-net exists                             : PASS
QEMU: Checking <span class="k">if </span>device /dev/net/tun exists                               : PASS
QEMU: Checking <span class="k">for </span>cgroup <span class="s1">'memory'</span> controller support                      : PASS
QEMU: Checking <span class="k">for </span>cgroup <span class="s1">'memory'</span> controller mount-point                  : PASS
QEMU: Checking <span class="k">for </span>cgroup <span class="s1">'cpu'</span> controller support                         : PASS
QEMU: Checking <span class="k">for </span>cgroup <span class="s1">'cpu'</span> controller mount-point                     : PASS
QEMU: Checking <span class="k">for </span>cgroup <span class="s1">'cpuacct'</span> controller support                     : PASS
QEMU: Checking <span class="k">for </span>cgroup <span class="s1">'cpuacct'</span> controller mount-point                 : PASS
QEMU: Checking <span class="k">for </span>cgroup <span class="s1">'cpuset'</span> controller support                      : PASS
QEMU: Checking <span class="k">for </span>cgroup <span class="s1">'cpuset'</span> controller mount-point                  : PASS
QEMU: Checking <span class="k">for </span>cgroup <span class="s1">'devices'</span> controller support                     : PASS
QEMU: Checking <span class="k">for </span>cgroup <span class="s1">'devices'</span> controller mount-point                 : PASS
QEMU: Checking <span class="k">for </span>cgroup <span class="s1">'blkio'</span> controller support                       : PASS
QEMU: Checking <span class="k">for </span>cgroup <span class="s1">'blkio'</span> controller mount-point                   : PASS
QEMU: Checking <span class="k">for </span>device assignment IOMMU support                         : PASS
 LXC: Checking <span class="k">for </span>Linux <span class="o">&gt;=</span> 2.6.26                                         : PASS
 LXC: Checking <span class="k">for </span>namespace ipc                                           : PASS
 LXC: Checking <span class="k">for </span>namespace mnt                                           : PASS
 LXC: Checking <span class="k">for </span>namespace pid                                           : PASS
 LXC: Checking <span class="k">for </span>namespace uts                                           : PASS
 LXC: Checking <span class="k">for </span>namespace net                                           : PASS
 LXC: Checking <span class="k">for </span>namespace user                                          : PASS
 LXC: Checking <span class="k">for </span>cgroup <span class="s1">'memory'</span> controller support                      : PASS
 LXC: Checking <span class="k">for </span>cgroup <span class="s1">'memory'</span> controller mount-point                  : PASS
 LXC: Checking <span class="k">for </span>cgroup <span class="s1">'cpu'</span> controller support                         : PASS
 LXC: Checking <span class="k">for </span>cgroup <span class="s1">'cpu'</span> controller mount-point                     : PASS
 LXC: Checking <span class="k">for </span>cgroup <span class="s1">'cpuacct'</span> controller support                     : PASS
 LXC: Checking <span class="k">for </span>cgroup <span class="s1">'cpuacct'</span> controller mount-point                 : PASS
 LXC: Checking <span class="k">for </span>cgroup <span class="s1">'cpuset'</span> controller support                      : PASS
 LXC: Checking <span class="k">for </span>cgroup <span class="s1">'cpuset'</span> controller mount-point                  : PASS
 LXC: Checking <span class="k">for </span>cgroup <span class="s1">'devices'</span> controller support                     : PASS
 LXC: Checking <span class="k">for </span>cgroup <span class="s1">'devices'</span> controller mount-point                 : PASS
 LXC: Checking <span class="k">for </span>cgroup <span class="s1">'blkio'</span> controller support                       : PASS
 LXC: Checking <span class="k">for </span>cgroup <span class="s1">'blkio'</span> controller mount-point                   : PASS
 LXC: Checking <span class="k">if </span>device /sys/fs/fuse/connections exists                   : PASS
</code></pre></div></div>

<h3 id="c-add-user-to-libvirt-groups">c. Add user to libvirt groups</h3>

<p>To allow the current user to manage the guest VM without sudo, we can add ourselves to all of the libvirt groups (e.g. libvirt, libvirt-qemu) and the kvm group</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cat</span> /etc/group | <span class="nb">grep </span>libvirt | <span class="nb">awk</span> <span class="nt">-F</span><span class="s1">':'</span> <span class="o">{</span><span class="s1">'print $1'</span><span class="o">}</span> | xargs <span class="nt">-n1</span> <span class="nb">sudo </span>adduser <span class="nv">$USER</span>

<span class="c"># add user to kvm group also</span>
<span class="nb">sudo </span>adduser <span class="nv">$USER</span> kvm

<span class="c"># relogin, then show group membership</span>
<span class="nb">exec </span>su <span class="nt">-l</span> <span class="nv">$USER</span>
<span class="nb">id</span> | <span class="nb">grep </span>libvirt
</code></pre></div></div>

<p>Group membership requires a user to log back in, so if the <code class="language-plaintext highlighter-rouge">id</code> command does not show your libvirt* group membership, logout and log back in, or try <code class="language-plaintext highlighter-rouge">exec su -l $USER</code>.</p>

<h3 id="d-qemu-connection-to-system">d. QEMU connection to system</h3>

<p>If not explicitly set, the userspace QEMU connection will be to <code class="language-plaintext highlighter-rouge">qemu:///session</code>, and not to <code class="language-plaintext highlighter-rouge">qemu:///system</code>.  This will cause you to see different domains, networks, and disk pool when executing virsh as your regular user versus sudo.</p>

<p>Modify your profile so that the environment variable below is exported to your login sessions.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># use same connection and objects as sudo</span>
<span class="nb">export </span><span class="nv">LIBVIRT_DEFAULT_URI</span><span class="o">=</span>qemu:///system
</code></pre></div></div>

<h3 id="e-default-network">e. Default network</h3>

<p>By default, KVM creates a virtual switch that shows up as a host interface named <code class="language-plaintext highlighter-rouge">virbr0</code> using 192.168.122.0/24.</p>

<p>This interface should be visible from the Host using the ‚Äúip‚Äù command below.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>ip addr show virbr0
3: virbr0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default qlen 1000
    <span class="nb">link</span>/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff
    inet 192.168.122.1/24 brd 192.168.122.255 scope global virbr0
       valid_lft forever preferred_lft forever
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">virbr0</code> operates in NAT mode, which allows the guest OS to communicate out, but only allowing the Host(and those VMs in its subnet) to make incoming connections.</p>

<h3 id="f-bridge-network">f. Bridge network</h3>

<p>To enable guest VMs on the same network as the Host, you should create a bridged network to your physical interface (e.g. eth0, ens4, epn1s0).</p>

<p>Read my article here for how to use NetPlan on Ubuntu to bridge your physical network interface to <code class="language-plaintext highlighter-rouge">br0</code> at the OS level.  And then use that to create a libvirt network named <code class="language-plaintext highlighter-rouge">host-bridge</code> that uses <code class="language-plaintext highlighter-rouge">br0</code>.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># bridge to physical network</span>
<span class="nv">$ </span>virsh net-dumpxml host-bridge

&lt;network <span class="nv">connections</span><span class="o">=</span><span class="s1">'2'</span><span class="o">&gt;</span>
  &lt;name&gt;host-bridge&lt;/name&gt;
  &lt;uuid&gt;44d2c3f5-6301-4fc6-be81-5ae2be4a47d8&lt;/uuid&gt;
  &lt;forward <span class="nv">mode</span><span class="o">=</span><span class="s1">'bridge'</span>/&gt;
  &lt;bridge <span class="nv">name</span><span class="o">=</span><span class="s1">'br0'</span>/&gt;
&lt;/network&gt;
</code></pre></div></div>

<p>This <code class="language-plaintext highlighter-rouge">host-bridge</code> will be required in later articles.</p>

<p>Instruction to setup host‚Äôs OS to create <code class="language-plaintext highlighter-rouge">br0</code> <a href="https://fabianlee.org/2019/04/01/kvm-creating-a-bridged-network-with-netplan-on-ubuntu-bionic/">here</a></p>

<h3 id="g-enable-ipv4-forwarding-on-kvm-host">g. Enable IPv4 forwarding on KVM host</h3>

<p>In order to handle NAT and routed networks for KVM, enable IPv4 forwarding on this host.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># this needs to be "1"</span>
<span class="nb">cat</span> /proc/sys/net/ipv4/ip_forward
<span class="c"># if not, then add it</span>
<span class="nb">echo </span>net.ipv4.ip_forward<span class="o">=</span>1 | <span class="nb">sudo tee</span> <span class="nt">-a</span> /etc/sysctl.conf

<span class="c"># make permanent</span>
<span class="nb">sudo </span>sysctl <span class="nt">-p</span> /etc/sysctl.conf
</code></pre></div></div>

<h3 id="h-default-storage-pool">h. Default storage pool</h3>

<p>The ‚Äúdefault‚Äù storage pool for guest disks is <code class="language-plaintext highlighter-rouge">/var/lib/libvirt/images</code>.   This is fine for test purposes, but if you have another mount that you want to use for guest OS disks, then you should create a custom storage pool.</p>

<p>Below are the commands to create a ‚Äúkvmpool‚Äù on an SSD mounted at <code class="language-plaintext highlighter-rouge">/data/kvm/pool</code>.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>virsh pool-list <span class="nt">--all</span>
 Name                 State      Autostart 
<span class="nt">-------------------------------------------</span>
 default              active     <span class="nb">yes</span>       

<span class="nv">$ </span>virsh pool-define-as kvmpool <span class="nt">--type</span> <span class="nb">dir</span> <span class="nt">--target</span> /data/kvm/pool
Pool kvmpool defined
<span class="nv">$ </span>virsh pool-list <span class="nt">--all</span>
<span class="nv">$ </span>virsh pool-start kvmpool
<span class="nv">$ </span>virsh pool-autostart kvmpool

<span class="nv">$ </span>virsh pool-list <span class="nt">--all</span>
 Name                 State      Autostart 
<span class="nt">-------------------------------------------</span>
 default              active     <span class="nb">yes       
 </span>kvmpool              active     <span class="nb">yes</span>
</code></pre></div></div>

<h2 id="4-vm-creation-using-virt-install">4. VM creation using <code class="language-plaintext highlighter-rouge">virt-install</code></h2>

<h3 id="a-download-ubuntu-2004-focal-iso">a. Download ubuntu 20.04 focal iso</h3>

<p>In order to test you need an OS boot image.  Since we are on an Ubuntu host, let‚Äôs download the ISO for the network installer of Ubuntu 20.04 Focal. When complete, you should have a local file named <code class="language-plaintext highlighter-rouge">~/kvm/mini.iso</code></p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>wget https://releases.ubuntu.com/20.04.6/ubuntu-20.04.6-desktop-amd64.iso <span class="nt">-O</span> ~/kvm/mini.iso
</code></pre></div></div>

<p>First list what virtual machines are running on our host:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># chown is only necessary if virsh was run previously as sudo</span>
<span class="nb">ls</span> <span class="nt">-l</span> ~/.virtinst
<span class="nb">sudo chown</span> <span class="nt">-R</span> <span class="nv">$USER</span>:<span class="nv">$USER</span> ~/.virtinst

<span class="c"># list VMs</span>
virsh list <span class="nt">--all</span>
</code></pre></div></div>

<p>This should return an empty list of VMs, because no guest OS have been deployed.</p>

<h3 id="b-installing-ukvm2004-vm">b. Installing <code class="language-plaintext highlighter-rouge">ukvm2004</code> VM</h3>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>virt-install <span class="nt">--virt-type</span><span class="o">=</span>kvm <span class="nt">--name</span><span class="o">=</span>ukvm2004 <span class="nt">--ram</span> 8192 <span class="nt">--vcpus</span><span class="o">=</span>4 <span class="nt">--virt-type</span><span class="o">=</span>kvm <span class="nt">--hvm</span> <span class="nt">--cdrom</span> ~/kvm/mini.iso <span class="nt">--network</span> <span class="nv">network</span><span class="o">=</span>default <span class="nt">--disk</span> <span class="nv">pool</span><span class="o">=</span>default,size<span class="o">=</span>20,bus<span class="o">=</span>virtio,format<span class="o">=</span>qcow2 <span class="nt">--noautoconsole</span> <span class="nt">--machine</span> q35
</code></pre></div></div>

<p>Note: When creating VM‚Äôs using virt-manager, make sure to also select <code class="language-plaintext highlighter-rouge">q35</code> as the machine type for full support of pcie in your guests.</p>

<ul>
  <li>VM name:   <code class="language-plaintext highlighter-rouge">ukvm2004</code></li>
  <li>VCPU: <code class="language-plaintext highlighter-rouge">4</code></li>
  <li>RAM:  <code class="language-plaintext highlighter-rouge">8G</code></li>
  <li>Network: <code class="language-plaintext highlighter-rouge">default virbr0 NAT network</code></li>
  <li>Pool storage:  <code class="language-plaintext highlighter-rouge">default</code> and size = 20GB</li>
  <li>Graphic: <code class="language-plaintext highlighter-rouge">default</code> - spice</li>
</ul>

<h3 id="c-open-the-vm">c. Open the VM</h3>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># open console to VM</span>
virt-viewer ukvm2004
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">virt-viewer</code> will popup a window for the Guest OS, when you click the mouse in the window and then press <ENTER> you will see the initial Ubuntu network install screen.</ENTER></p>

<p><code class="language-plaintext highlighter-rouge">virt-manager</code> provides a convenient interface for creating or managing a guest OS, and any guest OS you create from the CLI using virt-install will show up in this list also.</p>

<h3 id="d-stop-and-delete-vm">d. Stop and delete VM</h3>

<p>If you want to delete this guest OS completely, close the GUI window opened with virt-viewer, then use the following commands:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>virsh destroy ukvm2004
virsh undefine ukvm2004
</code></pre></div></div>

<h2 id="5-references">5. References</h2>

<p>Visit <a href="https://fabianlee.org/2018/08/27/kvm-bare-metal-virtualization-on-ubuntu-with-kvm/">kvm all commands</a>  and <a href="https://www.xilinx.com/developer/articles/using-alveo-data-center-accelerator-cards-in-a-kvm-environment.html">Xilinx instruction</a></p>
:ET